"""Model evaluation utilities for Lending Club credit risk experiments."""

from __future__ import annotations

from typing import Dict, Iterable, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import scipy.stats as sps
from lightgbm import LGBMClassifier
from sklearn.base import clone
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
    roc_curve,
)
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier

# Default random seed used across all estimators.  This keeps experiments reproducible
# while still allowing the caller to override the value before importing this module.
SEED = 42


def plot_confusion_matrix(
    mean_conf_matrix: np.ndarray,
    xtick_labels: Iterable[str] | None = None,
    ytick_labels: Iterable[str] | None = None,
) -> None:
    """Plot the confusion matrix as a heatmap."""

    fig, axes = plt.subplots(figsize=(8, 6))
    ax = sns.heatmap(
        mean_conf_matrix.T,
        annot=True,
        cmap="Blues",
        fmt=".1f",
        annot_kws={"size": 18},
    )
    if xtick_labels:
        ax.set_xticklabels(list(xtick_labels))
    if ytick_labels:
        ax.set_yticklabels(list(ytick_labels))

    ax.set_xlabel("True Label")
    ax.set_ylabel("Predicted Label")
    plt.title("Mean Confusion Matrix")
    plt.show()


def _train_with_optional_early_stopping(
    model, X_trn: pd.DataFrame, y_trn: pd.Series
) -> Tuple:
    """Clone and train a model, enabling early stopping where available.

    XGBoost and LightGBM benefit substantially from early stopping when working
    with the large Lending Club data set.  To avoid leaking information from the
    test split, we reserve a small portion of the training data for validation
    and only train on the remainder when the estimator exposes an early stopping
    interface.
    """

    estimator = clone(model)
    fit_X, fit_y = X_trn, y_trn
    fit_kwargs: Dict = {}

    if isinstance(estimator, XGBClassifier):
        fit_X, X_val, fit_y, y_val = train_test_split(
            X_trn,
            y_trn,
            test_size=0.1,
            stratify=y_trn,
            random_state=SEED,
        )
        fit_kwargs = {
            "eval_set": [(X_val, y_val)],
            "early_stopping_rounds": 50,
            "verbose": False,
        }
    elif isinstance(estimator, LGBMClassifier):
        fit_X, X_val, fit_y, y_val = train_test_split(
            X_trn,
            y_trn,
            test_size=0.1,
            stratify=y_trn,
            random_state=SEED,
        )
        fit_kwargs = {
            "eval_set": [(X_val, y_val)],
            "eval_metric": "auc",
            "early_stopping_rounds": 50,
            "verbose": -1,
        }

    estimator.fit(fit_X, fit_y, **fit_kwargs)
    return estimator


def calculate_scores(model, X_trn, y_trn, X_tst, y_tst):
    """Train a model and calculate various performance metrics on the test set."""

    estimator = _train_with_optional_early_stopping(model, X_trn, y_trn)
    y_pred = estimator.predict(X_tst)
    accuracy = accuracy_score(y_tst, y_pred)

    conf_matrix = confusion_matrix(y_tst, y_pred)
    precision = precision_score(y_tst, y_pred)
    recall = recall_score(y_tst, y_pred)
    f1 = f1_score(y_tst, y_pred)

    y_pred_proba = estimator.predict_proba(X_tst)[:, 1]
    auc = roc_auc_score(y_tst, y_pred_proba)
    fpr, tpr, _ = roc_curve(y_tst, y_pred_proba)

    mask = y_tst.astype(bool).values
    churn = y_pred_proba[mask]
    not_churn = y_pred_proba[~mask]
    ks = sps.ks_2samp(churn, not_churn)[0]

    return accuracy, auc, ks, conf_matrix, precision, recall, f1, fpr, tpr


def fit_models_summary(models, X_train, y_train, X_test, y_test):
    """Evaluate multiple models on pre-split training and testing data."""

    results = []
    roc_data = {}
    for name, model in models.items():
        print(f"Evaluating {name}...")
        (
            accuracy,
            auc,
            ks,
            conf_matrix,
            precision,
            recall,
            f1,
            fpr,
            tpr,
        ) = calculate_scores(model, X_train, y_train, X_test, y_test)

        results.append(
            {
                "Model": name,
                "Accuracy": accuracy,
                "AUC": auc,
                "KS": ks,
                "Precision": precision,
                "Recall": recall,
                "F1-Score": f1,
            }
        )
        roc_data[name] = (fpr, tpr, auc)
        print(f"Model: {name}")
        print(f"Confusion Matrix:\n{conf_matrix}\n")

    return pd.DataFrame(results), roc_data


def plot_roc_curves(roc_data: Dict[str, Tuple[np.ndarray, np.ndarray, float]]) -> None:
    """Plot ROC curves for all models on a single graph."""

    plt.figure(figsize=(10, 8))
    for model_name, (fpr, tpr, auc) in roc_data.items():
        plt.plot(fpr, tpr, label=f"{model_name} (AUC = {auc:.3f})")
    plt.plot([0, 1], [0, 1], "k--", label="Random Guess (AUC = 0.5)")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curves for Loan Default Prediction Models")
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()


# Tuned model configurations with stronger depth/regularisation trade-offs.
models = {
    "Logistic Regression": Pipeline(
        steps=[
            ("scaler", StandardScaler()),
            (
                "model",
                LogisticRegression(
                    max_iter=1000,
                    class_weight="balanced",
                    random_state=SEED,
                ),
            ),
        ]
    ),
    "Decision Tree": DecisionTreeClassifier(
        max_depth=12,
        min_samples_leaf=50,
        random_state=SEED,
    ),
    "K Nearest Neighbors": Pipeline(
        steps=[
            ("scaler", StandardScaler()),
            ("model", KNeighborsClassifier(n_neighbors=50, weights="distance")),
        ]
    ),
    "Random Forest": RandomForestClassifier(
        n_estimators=400,
        max_depth=12,
        min_samples_leaf=50,
        n_jobs=-1,
        random_state=SEED,
    ),
    "Gaussian Naive Bayes": GaussianNB(),
    "Gradient Boosting": GradientBoostingClassifier(
        n_estimators=400,
        learning_rate=0.05,
        max_depth=3,
        random_state=SEED,
    ),
    "Light GBM": LGBMClassifier(
        n_estimators=1000,
        learning_rate=0.05,
        num_leaves=63,
        max_depth=-1,
        subsample=0.8,
        subsample_freq=1,
        colsample_bytree=0.8,
        min_child_samples=100,
        reg_lambda=1.0,
        random_state=SEED,
        n_jobs=-1,
    ),
    "XGBoost": XGBClassifier(
        n_estimators=800,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=3,
        reg_lambda=1.0,
        tree_method="hist",
        n_jobs=-1,
        eval_metric="auc",
        random_state=SEED,
        use_label_encoder=False,
    ),
    "Neural Network": Pipeline(
        steps=[
            ("scaler", StandardScaler()),
            (
                "mlp",
                MLPClassifier(
                    hidden_layer_sizes=(128, 64, 32),
                    activation="relu",
                    alpha=1e-3,
                    batch_size=512,
                    learning_rate_init=0.001,
                    max_iter=200,
                    early_stopping=True,
                    validation_fraction=0.1,
                    n_iter_no_change=15,
                    random_state=SEED,
                ),
            ),
        ]
    ),
}

